{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kristina-26/DEEP-LEARNING-TASK-2/blob/main/DeepLearningTask2_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFy2lyQQTMr5"
      },
      "outputs": [],
      "source": [
        "# Kristina KazlauskaitÄ—\n",
        "# LSP: S2416112\n",
        "# full-time studies\n",
        "# task: realization a semantic search tool for the selected context using a natural language model-based transformer neural networks to be used as a feature\n",
        "#       extractor and save representations to vector database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1Uc9M7buaGo"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q beautifulsoup4 requests transformers sentence-transformers faiss-cpu pandas numpy matplotlib seaborn parsel torch\n",
        "!pip install datasets accelerate scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUb1M4cjudmD"
      },
      "outputs": [],
      "source": [
        "import httpx\n",
        "from parsel import Selector\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import json\n",
        "from datetime import datetime\n",
        "import torch\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import AutoModel, AutoConfig\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "from transformers import BertModel, BertTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdKWwW1HueXx"
      },
      "outputs": [],
      "source": [
        "# set up a custom HTTP2 client to simulate how a real web browser (in this case Edge) interacts with a website\n",
        "session = httpx.Client(\n",
        "    headers={\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36 Edg/113.0.1774.35\",\n",
        "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n",
        "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "        \"Accept-Encoding\": \"gzip, deflate, br\", # compressed responses\n",
        "    },\n",
        "    http2=True, # faster than http1\n",
        "    follow_redirects=True,\n",
        "    timeout=30.0\n",
        ")\n",
        "\n",
        "# scraping listings in diverse categories\n",
        "search_urls = [\n",
        "    \"https://www.ebay.com/sch/i.html?_nkw=electronics\",\n",
        "    \"https://www.ebay.com/sch/i.html?_nkw=laptops\",\n",
        "    \"https://www.ebay.com/sch/i.html?_nkw=cameras\",\n",
        "    \"https://www.ebay.com/sch/i.html?_nkw=home+garden\",\n",
        "    \"https://www.ebay.com/sch/i.html?_nkw=sporting+goods\",\n",
        "    \"https://www.ebay.com/sch/i.html?_nkw=clothing\",\n",
        "    \"https://www.ebay.com/sch/i.html?_nkw=jewelry\",\n",
        "    \"https://www.ebay.com/sch/i.html?_nkw=video+games\",\n",
        "    \"https://www.ebay.com/sch/i.html?_nkw=books\",\n",
        "    \"https://www.ebay.com/sch/i.html?_nkw=collectibles\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZZnqZSyufWT"
      },
      "outputs": [],
      "source": [
        "#  extract product info from a listing page with parsel - return a list of product dictionaries\n",
        "def extract_product_info_httpx(selector):\n",
        "    products = []\n",
        "\n",
        "    # find all product listings\n",
        "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Looking for product listings...\")\n",
        "    listings = selector.css('li.s-item')\n",
        "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Found {len(listings)} listings\")\n",
        "\n",
        "    for i, listing in enumerate(listings):\n",
        "        try:\n",
        "            if i < 5:  # show details for first 5 items\n",
        "                print(f\"[{datetime.now().strftime('%H:%M:%S')}] Processing product {i+1}\")\n",
        "\n",
        "            # title\n",
        "            title = listing.css('.s-item__title span::text').get('').strip()\n",
        "\n",
        "            # price\n",
        "            price = listing.css('.s-item__price::text').get('').strip()\n",
        "\n",
        "            # condition\n",
        "            condition = listing.css('.SECONDARY_INFO::text').get('').strip()\n",
        "\n",
        "            # link\n",
        "            link = listing.css('.s-item__link::attr(href)').get('')\n",
        "\n",
        "            # image\n",
        "            image_url = listing.css('.s-item__image img::attr(src)').get('')\n",
        "\n",
        "            # location\n",
        "            location = listing.css('.s-item__location::text').get('').strip()\n",
        "\n",
        "            # description (if available)\n",
        "            description = listing.css('.s-item__subtitle::text').get('').strip()\n",
        "\n",
        "            if i < 5:  # show details for first 5 items\n",
        "                print(f\"  Title: {title[:50]}...\")\n",
        "                print(f\"  Price: {price}\")\n",
        "\n",
        "            products.append({\n",
        "                'title': title,\n",
        "                'price': price,\n",
        "                'condition': condition,\n",
        "                'link': link,\n",
        "                'image_url': image_url,\n",
        "                'location': location,\n",
        "                'description': description,\n",
        "                'timestamp': datetime.now().isoformat()\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            if i < 5:  # show errors for first 5 products\n",
        "                print(f\"Error extracting product {i+1}: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Successfully extracted {len(products)} products\")\n",
        "    return products # a list of dictionaries\n",
        "\n",
        "# scrape products from a search URL using httpx from pages 1 to max_pages\n",
        "def scrape_ebay_search(base_url, max_pages=5):\n",
        "    all_products = []\n",
        "\n",
        "    for page in range(1, max_pages + 1):\n",
        "        try:\n",
        "            # pagination parameter\n",
        "            page_url = f\"{base_url}&_pgn={page}\"\n",
        "\n",
        "            print(f\"\\n{'*'*20} PAGE {page} {'*'*20}\")\n",
        "            print(f\"Scraping page {page} of {max_pages}\")\n",
        "            print(f\"URL: {page_url[:80]}...\")\n",
        "\n",
        "            # make request with retry logic\n",
        "            print(f\"[{datetime.now().strftime('%H:%M:%S')}] Sending request...\")\n",
        "\n",
        "            max_retries = 3 # retry up to 3 times if it times out\n",
        "            for attempt in range(max_retries):\n",
        "                try:\n",
        "                    response = session.get(page_url)\n",
        "                    response.raise_for_status()\n",
        "                    break\n",
        "                except httpx.TimeoutException:\n",
        "                    if attempt < max_retries - 1:\n",
        "                        wait_time = 5 * (attempt + 1)\n",
        "                        print(f\"Timeout, retrying in {wait_time} seconds...\")\n",
        "                        time.sleep(wait_time)\n",
        "                    else:\n",
        "                        raise\n",
        "\n",
        "            print(f\"[{datetime.now().strftime('%H:%M:%S')}] Response received. Status code: {response.status_code}\")\n",
        "\n",
        "            # parse HTML\n",
        "            print(f\"[{datetime.now().strftime('%H:%M:%S')}] Parsing HTML...\")\n",
        "            selector = Selector(response.text)\n",
        "\n",
        "            # extract products\n",
        "            print(f\"[{datetime.now().strftime('%H:%M:%S')}] Extracting products...\")\n",
        "            products = extract_product_info_httpx(selector)\n",
        "            all_products.extend(products)\n",
        "\n",
        "            print(f\"[{datetime.now().strftime('%H:%M:%S')}] Extracted {len(products)} products from page {page}\")\n",
        "            print(f\"[{datetime.now().strftime('%H:%M:%S')}] Total products so far: {len(all_products)}\")\n",
        "\n",
        "            # use random delay between page requests\n",
        "            wait_time = random.uniform(3, 7)\n",
        "            print(f\"[{datetime.now().strftime('%H:%M:%S')}] Waiting {wait_time:.1f} seconds before next request...\")\n",
        "            time.sleep(wait_time)\n",
        "\n",
        "        except httpx.TimeoutException:\n",
        "            print(f\"[{datetime.now().strftime('%H:%M:%S')}] Timeout error on page {page}\")\n",
        "            # try to continue with next page\n",
        "            continue\n",
        "        except httpx.HTTPStatusError as e:\n",
        "            if e.response.status_code == 429:\n",
        "                print(f\"[{datetime.now().strftime('%H:%M:%S')}] Rate limited. Waiting 30 seconds...\")\n",
        "                time.sleep(30)\n",
        "                continue\n",
        "            else:\n",
        "                print(f\"[{datetime.now().strftime('%H:%M:%S')}] HTTP error on page {page}: {e}\")\n",
        "                continue\n",
        "        except Exception as e:\n",
        "            print(f\"[{datetime.now().strftime('%H:%M:%S')}] ERROR on page {page}: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            continue\n",
        "\n",
        "    return all_products"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qLMu_s0sugd2"
      },
      "outputs": [],
      "source": [
        "# collect at least 5000 entries\n",
        "all_products = []\n",
        "target_products = 5000\n",
        "\n",
        "# loop through each category\n",
        "for idx, search_url in enumerate(search_urls):\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Scraping search query {idx+1}/{len(search_urls)}\")\n",
        "    print(f\"URL: {search_url}\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    # scrape products from this category\n",
        "    products = scrape_ebay_search(search_url, max_pages=10)\n",
        "    all_products.extend(products)\n",
        "\n",
        "    print(f\"Total products collected so far: {len(all_products)}\")\n",
        "\n",
        "    # step after reaching the target\n",
        "    if len(all_products) >= target_products:\n",
        "        print(f\"\\nReached target of {target_products} products!\")\n",
        "        break\n",
        "\n",
        "    # save intermediate results in a CSV file\n",
        "    df_intermediate = pd.DataFrame(all_products)\n",
        "    df_intermediate.to_csv(f'ebay_products_intermediate_{idx+1}.csv', index=False)\n",
        "\n",
        "    # longer random delay between different searches\n",
        "    wait_time = random.uniform(10, 15)\n",
        "    print(f\"Waiting {wait_time:.1f} seconds before next search query...\")\n",
        "    time.sleep(wait_time)\n",
        "\n",
        "# final DataFrame\n",
        "df = pd.DataFrame(all_products)\n",
        "print(f\"\\nFinal dataset shape: {df.shape}\")\n",
        "print(f\"Total products collected: {len(df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "F9jiS9WLuh3O"
      },
      "outputs": [],
      "source": [
        "# remove duplicates\n",
        "df = df.drop_duplicates(subset=['title', 'price', 'link'])\n",
        "print(f\"After removing duplicates: {len(df)} products\")\n",
        "\n",
        "# create a combined text field for better search\n",
        "df['combined_text'] = df.apply(lambda row: f\"{row['title']} {row['description']} {row['condition']}\", axis=1)\n",
        "\n",
        "# first few entries\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NV0m_RK9ui--"
      },
      "outputs": [],
      "source": [
        "# save dataset into a CSV\n",
        "df.to_csv('ebay_products_dataset.csv', index=False)\n",
        "print(\"Dataset saved to 'ebay_products_dataset.csv'\")\n",
        "\n",
        "# save a text file with just the combined text for semantic search\n",
        "with open('ebay_products_text.txt', 'w', encoding='utf-8') as f:\n",
        "    for text in df['combined_text']:\n",
        "        f.write(text + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NtEIODQuk58"
      },
      "outputs": [],
      "source": [
        "# load a pre-trained sentence transformer model for embedding\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "print(\"Sentence transformer model loaded successfully\")\n",
        "\n",
        "# test the model\n",
        "test_embedding = model.encode(\"test sentence\")\n",
        "print(f\"Embedding dimension: {test_embedding.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGvSZ_lgul_K"
      },
      "outputs": [],
      "source": [
        "# create embeddings for all products\n",
        "print(\"Creating embeddings for all products...\")\n",
        "embeddings = model.encode(df['combined_text'].tolist(), show_progress_bar=True)\n",
        "print(f\"Embeddings shape: {embeddings.shape}\")\n",
        "\n",
        "# save embeddings (NumPy array) to reload later\n",
        "np.save('ebay_product_embeddings.npy', embeddings)\n",
        "print(\"Embeddings saved to 'ebay_product_embeddings.npy'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URs5ydSFum9i"
      },
      "outputs": [],
      "source": [
        "# create FAISS index that uses L2 distance (Euclidean) to measure similarity between vectors\n",
        "dimension = embeddings.shape[1]\n",
        "print(dimanesion) # for all-MiniLM-L6-v2 this is 384\n",
        "index = faiss.IndexFlatL2(dimension) # embeddings are normalized, so L2 distance can still reflect semantic similarity\n",
        "\n",
        "# add item embeddings to the index\n",
        "index.add(embeddings.astype('float32'))\n",
        "\n",
        "# save the index to a binary file for reloading later\n",
        "faiss.write_index(index, \"ebay_product_index.bin\")\n",
        "print(f\"FAISS index created with {index.ntotal} vectors\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdRRq1OmIQbo"
      },
      "source": [
        "# RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUAQXC5kLr-Z"
      },
      "outputs": [],
      "source": [
        "# search system that uses RAG for query enhancement and result refinement\n",
        "class RAGEnhancedSearchSystem:\n",
        "    def __init__(self, embedding_model, index, df, generator_model_name=\"google/flan-t5-large\"):\n",
        "        self.embedding_model = embedding_model\n",
        "        self.index = index\n",
        "        self.df = df\n",
        "\n",
        "        # initialize generative model for RAG-based search enhancement\n",
        "        print(f\"Loading generative model for RAG search: {generator_model_name}\")\n",
        "        self.generator_tokenizer = T5Tokenizer.from_pretrained(generator_model_name)\n",
        "        self.generator_model = T5ForConditionalGeneration.from_pretrained(generator_model_name)\n",
        "\n",
        "        # move to GPU if available\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.generator_model = self.generator_model.to(self.device)\n",
        "        print(f\"Model loaded on {self.device}\")\n",
        "\n",
        "    # use RAG to expand the search query for better retrieval\n",
        "    def rag_query_expansion(self, original_query):\n",
        "\n",
        "        expansion_terms = []\n",
        "\n",
        "        prompt = f\"\"\"Generate 3-5 related search terms for this eBay search query.\n",
        "Only output the new terms, separated by commas. Do not repeat the original query.\n",
        "\n",
        "Query: {original_query}\n",
        "\n",
        "Related terms:\"\"\"\n",
        "\n",
        "        inputs = self.generator_tokenizer(\n",
        "            prompt,\n",
        "            max_length=128,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(self.device)\n",
        "\n",
        "        outputs = self.generator_model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_length=50,\n",
        "            num_beams=4,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "        enhanced_terms = self.generator_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        if enhanced_terms and enhanced_terms.lower() != original_query.lower():\n",
        "            expansion_terms = enhanced_terms.split(',')\n",
        "\n",
        "        # clean and format expansion terms\n",
        "        cleaned_terms = [term.strip() for term in expansion_terms if term.strip()]\n",
        "        expansion_text = ' '.join(cleaned_terms[:3])  # use top 3 terms\n",
        "\n",
        "        # combine original query with expansion terms\n",
        "        if expansion_text:\n",
        "            combined_query = f\"{original_query} {expansion_text}\"\n",
        "        else:\n",
        "            combined_query = original_query\n",
        "\n",
        "        return combined_query, expansion_text\n",
        "\n",
        "    # use RAG to rerank search results based on relevance\n",
        "    def rag_result_reranking(self, original_query, search_results):\n",
        "        # prepare context with search results\n",
        "        context = \"Search results to analyze:\\n\\n\"\n",
        "        for i, result in enumerate(search_results, 1):\n",
        "            context += f\"Result {i}:\\n\"\n",
        "            context += f\"Title: {result['title']}\\n\"\n",
        "            context += f\"Price: {result['price']}\\n\"\n",
        "            if 'condition' in result:\n",
        "                context += f\"Condition: {result['condition']}\\n\"\n",
        "            if 'description' in result and result['description']:\n",
        "                context += f\"Description: {result['description'][:100]}...\\n\"\n",
        "            context += \"\\n\"\n",
        "\n",
        "        prompt = f\"\"\"You are a search result analyzer. Rerank these eBay search results based on relevance to the query.\n",
        "\n",
        "Query: {original_query}\n",
        "\n",
        "{context}\n",
        "\n",
        "Instructions:\n",
        "1. Analyze each result's relevance to the query\n",
        "2. Consider product specifics, price range, and condition\n",
        "3. Output the ranking as numbers 1,2,3... in order of relevance\n",
        "\n",
        "Result ranking (most relevant first):\"\"\"\n",
        "\n",
        "        inputs = self.generator_tokenizer(\n",
        "            prompt,\n",
        "            max_length=512,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(self.device)\n",
        "\n",
        "        outputs = self.generator_model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_length=50,\n",
        "            num_beams=4,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "        ranking_text = self.generator_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # parse ranking\n",
        "        try:\n",
        "            rankings = [int(x) for x in ranking_text.split(',')]\n",
        "            reranked_results = []\n",
        "            for rank in rankings:\n",
        "                if 1 <= rank <= len(search_results):\n",
        "                    result = search_results[rank - 1].copy()\n",
        "                    result['rag_rank'] = len(reranked_results) + 1\n",
        "                    reranked_results.append(result)\n",
        "\n",
        "            # add any missing results\n",
        "            for i, result in enumerate(search_results):\n",
        "                if not any(r['title'] == result['title'] for r in reranked_results):\n",
        "                    result_copy = result.copy()\n",
        "                    result_copy['rag_rank'] = len(reranked_results) + 1\n",
        "                    reranked_results.append(result_copy)\n",
        "\n",
        "            return reranked_results\n",
        "        except:\n",
        "            # fallback to original order if parsing fails\n",
        "            print(\"Failed to parse RAG ranking, using original order\")\n",
        "            return search_results\n",
        "\n",
        "    # perform search using RAG for query enhancement and result reranking\n",
        "    def rag_enhanced_search(self, query, top_k=5):\n",
        "        print(f\"\\nRAG-ENHANCED SEARCH PROCESS:\")\n",
        "        print(f\"Original query: '{query}'\")\n",
        "\n",
        "        # 1. RAG Query Expansion\n",
        "        expanded_query, enhancement_terms = self.rag_query_expansion(query)\n",
        "        print(f\"Enhanced query: '{expanded_query}'\")\n",
        "        print(f\"Enhancement terms: '{enhancement_terms}'\")\n",
        "\n",
        "        # 2. Search with expanded query\n",
        "        query_embedding = self.embedding_model.encode([expanded_query])\n",
        "        distances, indices = self.index.search(query_embedding.astype('float32'), top_k * 2)  # get extra results\n",
        "\n",
        "        # 3. format initial results\n",
        "        initial_results = []\n",
        "        for i, idx in enumerate(indices[0]):\n",
        "            if idx != -1:\n",
        "                product = self.df.iloc[idx]\n",
        "                initial_results.append({\n",
        "                    'rank': i + 1,\n",
        "                    'title': product['title'],\n",
        "                    'price': product['price'],\n",
        "                    'condition': product['condition'],\n",
        "                    'description': product['description'],\n",
        "                    'link': product['link'],\n",
        "                    'similarity_score': 1 / (1 + distances[0][i])\n",
        "                })\n",
        "\n",
        "        # 4. RAG-based reranking\n",
        "        reranked_results = self.rag_result_reranking(query, initial_results)\n",
        "\n",
        "        # return top_k results\n",
        "        return reranked_results[:top_k]\n",
        "\n",
        "    # complete RAG pipeline: enhanced search + context-aware generation\n",
        "    def full_rag_pipeline(self, query, top_k=5):\n",
        "        # 1. RAG-enhanced search\n",
        "        search_results = self.rag_enhanced_search(query, top_k=top_k)\n",
        "\n",
        "        # 2. generate response using retrieved context\n",
        "        generated_response = self.generate_response(query, search_results)\n",
        "\n",
        "        return {\n",
        "            'query': query,\n",
        "            'search_results': search_results,\n",
        "            'generated_response': generated_response\n",
        "        }\n",
        "\n",
        "    # generate response using retrieved context\n",
        "    def generate_response(self, query, search_results):\n",
        "        \"\"\"Generate a helpful, concrete response using retrieved search results\"\"\"\n",
        "\n",
        "        # create context from search results\n",
        "        context = \"Top search results for your query:\\n\\n\"\n",
        "        for i, result in enumerate(search_results, 1):\n",
        "            context += f\"Product {i} (RAG Rank: {result.get('rag_rank', i)}):\\n\"\n",
        "            context += f\"- Title: {result['title']}\\n\"\n",
        "            context += f\"- Price: {result['price']}\\n\"\n",
        "            if 'condition' in result:\n",
        "                context += f\"- Condition: {result['condition']}\\n\"\n",
        "            if 'description' in result and result['description']:\n",
        "                context += f\"- Description: {result['description'][:150]}...\\n\"\n",
        "            context += f\"- Match quality: {result['similarity_score']:.2f}\\n\\n\"\n",
        "\n",
        "        # prompt with instructions\n",
        "        prompt = f\"\"\"Recommend the best product for: {query}\n",
        "\n",
        "Products available:\n",
        "{context}\n",
        "\n",
        "Write a brief recommendation. Begin with \"Based on your search for {query},\" then mention the best product by name and price. Explain why it's good. Compare to one alternative if relevant.\n",
        "\"\"\"\n",
        "\n",
        "        inputs = self.generator_tokenizer(\n",
        "            prompt,\n",
        "            max_length=768,  # to give model more context\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(self.device)\n",
        "\n",
        "        # output parameters\n",
        "        outputs = self.generator_model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_length=400,\n",
        "            min_length=100,\n",
        "            length_penalty=1.5,   # encourage slightly longer responses\n",
        "            num_beams=5,          # beam search paths\n",
        "            temperature=0.8,      # add creativity\n",
        "            early_stopping=True,\n",
        "            no_repeat_ngram_size=3  # prevent repetitive text\n",
        "        )\n",
        "\n",
        "        response = self.generator_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        return response\n",
        "\n",
        "    # demonstrate the RAG-enhanced search process step by step\n",
        "    def demonstrate_rag_search_process(self, query):\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(f\"DEMONSTRATING RAG-ENHANCED SEARCH PROCESS\")\n",
        "        print(f\"Query: '{query}'\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # 1. query enhancement\n",
        "        expanded_query, enhancement_terms = self.rag_query_expansion(query)\n",
        "        print(\"\\n1. RAG QUERY ENHANCEMENT:\")\n",
        "        print(f\"   Original: '{query}'\")\n",
        "        print(f\"   Enhanced: '{expanded_query}'\")\n",
        "        print(f\"   Added terms: '{enhancement_terms}'\")\n",
        "\n",
        "        # 2. initial search\n",
        "        print(\"\\n2. VECTOR SEARCH WITH ENHANCED QUERY:\")\n",
        "        query_embedding = self.embedding_model.encode([expanded_query])\n",
        "        distances, indices = self.index.search(query_embedding.astype('float32'), 5)\n",
        "\n",
        "        initial_results = []\n",
        "        for i, idx in enumerate(indices[0]):\n",
        "            if idx != -1:\n",
        "                product = self.df.iloc[idx]\n",
        "                result = {\n",
        "                    'rank': i + 1,\n",
        "                    'title': product['title'],\n",
        "                    'price': product['price'],\n",
        "                    'similarity_score': 1 / (1 + distances[0][i])\n",
        "                }\n",
        "                initial_results.append(result)\n",
        "                print(f\"   {i+1}. {product['title'][:50]}...\")\n",
        "                print(f\"      Price: {product['price']} | Similarity: {result['similarity_score']:.3f}\")\n",
        "\n",
        "        # 3.  RAG reranking\n",
        "        print(\"\\n3. RAG-BASED RERANKING:\")\n",
        "        reranked_results = self.rag_result_reranking(query, initial_results)\n",
        "        for i, result in enumerate(reranked_results, 1):\n",
        "            print(f\"   {i}. (Original rank: {result['rank']}, New RAG rank: {result.get('rag_rank', i)})\")\n",
        "            print(f\"      {result['title'][:50]}...\")\n",
        "            print(f\"      Price: {result['price']}\")\n",
        "\n",
        "        # 4. generate final response\n",
        "        print(\"\\n4. GENERATE FINAL RESPONSE:\")\n",
        "        response = self.generate_response(query, reranked_results)\n",
        "        print(\"\\nAI Response:\")\n",
        "        print(\"-\" * 50)\n",
        "        print(response)\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        return reranked_results\n",
        "\n",
        "# create the RAG-enhanced search system\n",
        "rag_search_system = RAGEnhancedSearchSystem(model, index, df, generator_model_name=\"google/flan-t5-large\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3oKOKZFLyV2"
      },
      "outputs": [],
      "source": [
        "# test the system with demonstration\n",
        "test_queries = [\n",
        "    \"I want to buy a gaming laptop\",\n",
        "    \"Can you help me find the best basketball shoes?\",\n",
        "    \"Help me pick golden earrings\"\n",
        "]\n",
        "\n",
        "print(\"\\nTESTING RAG-ENHANCED SEARCH SYSTEM\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for query in test_queries:\n",
        "    results = rag_search_system.demonstrate_rag_search_process(query)\n",
        "    print(\"\\n\" + \"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sN3lrlmbMMdV"
      },
      "outputs": [],
      "source": [
        "# interactive RAG-enhanced search\n",
        "def interactive_rag_search_demo():\n",
        "    \"\"\"Interactive demonstration of RAG-enhanced search\"\"\"\n",
        "    print(\"\\nINTERACTIVE RAG-ENHANCED SEARCH DEMO\")\n",
        "    print(\"Type 'quit' to exit\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    while True:\n",
        "        query = input(\"\\nEnter your search query: \").strip()\n",
        "\n",
        "        if query.lower() in ['quit', 'exit', \"q\"]:\n",
        "            break\n",
        "\n",
        "        if query:\n",
        "            rag_search_system.demonstrate_rag_search_process(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFecFh2MMX8f"
      },
      "outputs": [],
      "source": [
        "# run interactive demo\n",
        "interactive_rag_search_demo()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMAwo4zYrbXHfE1ttylYyyW",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}