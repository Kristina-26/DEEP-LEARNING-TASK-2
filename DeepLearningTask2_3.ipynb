{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kristina-26/DEEP-LEARNING-TASK-2/blob/main/DeepLearningTask2_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFy2lyQQTMr5"
      },
      "outputs": [],
      "source": [
        "# Kristina KazlauskaitÄ—\n",
        "# LSP: S2416112\n",
        "# full-time studies\n",
        "# task: realization a semantic search tool for the selected context using a natural language model-based transformer neural networks to be used as a feature\n",
        "#       extractor and save representations to vector database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1Uc9M7buaGo"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q beautifulsoup4 requests transformers sentence-transformers faiss-cpu pandas numpy matplotlib seaborn parsel torch\n",
        "# !pip install datasets accelerate scikit-learn transformers sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUb1M4cjudmD"
      },
      "outputs": [],
      "source": [
        "import httpx\n",
        "from parsel import Selector\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import json\n",
        "from datetime import datetime\n",
        "import torch\n",
        "# from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import AutoModel, AutoConfig\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "from transformers import BertModel, BertTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdKWwW1HueXx"
      },
      "outputs": [],
      "source": [
        "# set up a custom HTTP2 client to simulate how a real web browser (in this case Edge) interacts with a website\n",
        "session = httpx.Client(\n",
        "    headers={\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36 Edg/113.0.1774.35\",\n",
        "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n",
        "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "        \"Accept-Encoding\": \"gzip, deflate, br\", # compressed responses\n",
        "    },\n",
        "    http2=True, # faster than http1\n",
        "    follow_redirects=True,\n",
        "    timeout=30.0\n",
        ")\n",
        "\n",
        "# scraping listings in diverse categories\n",
        "search_urls = [\n",
        "    \"https://www.ebay.com/sch/i.html?_nkw=electronics\",\n",
        "    \"https://www.ebay.com/sch/i.html?_nkw=laptops\",\n",
        "    \"https://www.ebay.com/sch/i.html?_nkw=cameras\",\n",
        "    \"https://www.ebay.com/sch/i.html?_nkw=home+garden\",\n",
        "    \"https://www.ebay.com/sch/i.html?_nkw=sporting+goods\",\n",
        "    \"https://www.ebay.com/sch/i.html?_nkw=clothing\",\n",
        "    \"https://www.ebay.com/sch/i.html?_nkw=jewelry\",\n",
        "    \"https://www.ebay.com/sch/i.html?_nkw=video+games\",\n",
        "    \"https://www.ebay.com/sch/i.html?_nkw=books\",\n",
        "    \"https://www.ebay.com/sch/i.html?_nkw=collectibles\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZZnqZSyufWT"
      },
      "outputs": [],
      "source": [
        "#  extract product info from a listing page with parsel - return a list of product dictionaries\n",
        "def extract_product_info_httpx(selector):\n",
        "    products = []\n",
        "\n",
        "    # find all product listings\n",
        "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Looking for product listings...\")\n",
        "    listings = selector.css('li.s-item')\n",
        "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Found {len(listings)} listings\")\n",
        "\n",
        "    for i, listing in enumerate(listings):\n",
        "        try:\n",
        "            if i < 5:  # show details for first 5 items\n",
        "                print(f\"[{datetime.now().strftime('%H:%M:%S')}] Processing product {i+1}\")\n",
        "\n",
        "            # title\n",
        "            title = listing.css('.s-item__title span::text').get('').strip()\n",
        "\n",
        "            # price\n",
        "            price = listing.css('.s-item__price::text').get('').strip()\n",
        "\n",
        "            # condition\n",
        "            condition = listing.css('.SECONDARY_INFO::text').get('').strip()\n",
        "\n",
        "            # link\n",
        "            link = listing.css('.s-item__link::attr(href)').get('')\n",
        "\n",
        "            # image\n",
        "            image_url = listing.css('.s-item__image img::attr(src)').get('')\n",
        "\n",
        "            # location\n",
        "            location = listing.css('.s-item__location::text').get('').strip()\n",
        "\n",
        "            # description (if available)\n",
        "            description = listing.css('.s-item__subtitle::text').get('').strip()\n",
        "\n",
        "            if i < 5:  # show details for first 5 items\n",
        "                print(f\"  Title: {title[:50]}...\")\n",
        "                print(f\"  Price: {price}\")\n",
        "\n",
        "            products.append({\n",
        "                'title': title,\n",
        "                'price': price,\n",
        "                'condition': condition,\n",
        "                'link': link,\n",
        "                'image_url': image_url,\n",
        "                'location': location,\n",
        "                'description': description,\n",
        "                'timestamp': datetime.now().isoformat()\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            if i < 5:  # show errors for first 5 products\n",
        "                print(f\"Error extracting product {i+1}: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Successfully extracted {len(products)} products\")\n",
        "    return products # a list of dictionaries\n",
        "\n",
        "# scrape products from a search URL using httpx from pages 1 to max_pages\n",
        "def scrape_ebay_search(base_url, max_pages=5):\n",
        "    all_products = []\n",
        "\n",
        "    for page in range(1, max_pages + 1):\n",
        "        try:\n",
        "            # pagination parameter\n",
        "            page_url = f\"{base_url}&_pgn={page}\"\n",
        "\n",
        "            print(f\"\\n{'*'*20} PAGE {page} {'*'*20}\")\n",
        "            print(f\"Scraping page {page} of {max_pages}\")\n",
        "            print(f\"URL: {page_url[:80]}...\")\n",
        "\n",
        "            # make request with retry logic\n",
        "            print(f\"[{datetime.now().strftime('%H:%M:%S')}] Sending request...\")\n",
        "\n",
        "            max_retries = 3 # retry up to 3 times if it times out\n",
        "            for attempt in range(max_retries):\n",
        "                try:\n",
        "                    response = session.get(page_url)\n",
        "                    response.raise_for_status()\n",
        "                    break\n",
        "                except httpx.TimeoutException:\n",
        "                    if attempt < max_retries - 1:\n",
        "                        wait_time = 5 * (attempt + 1)\n",
        "                        print(f\"Timeout, retrying in {wait_time} seconds...\")\n",
        "                        time.sleep(wait_time)\n",
        "                    else:\n",
        "                        raise\n",
        "\n",
        "            print(f\"[{datetime.now().strftime('%H:%M:%S')}] Response received. Status code: {response.status_code}\")\n",
        "\n",
        "            # parse HTML\n",
        "            print(f\"[{datetime.now().strftime('%H:%M:%S')}] Parsing HTML...\")\n",
        "            selector = Selector(response.text)\n",
        "\n",
        "            # extract products\n",
        "            print(f\"[{datetime.now().strftime('%H:%M:%S')}] Extracting products...\")\n",
        "            products = extract_product_info_httpx(selector)\n",
        "            all_products.extend(products)\n",
        "\n",
        "            print(f\"[{datetime.now().strftime('%H:%M:%S')}] Extracted {len(products)} products from page {page}\")\n",
        "            print(f\"[{datetime.now().strftime('%H:%M:%S')}] Total products so far: {len(all_products)}\")\n",
        "\n",
        "            # use random delay between page requests\n",
        "            wait_time = random.uniform(3, 7)\n",
        "            print(f\"[{datetime.now().strftime('%H:%M:%S')}] Waiting {wait_time:.1f} seconds before next request...\")\n",
        "            time.sleep(wait_time)\n",
        "\n",
        "        except httpx.TimeoutException:\n",
        "            print(f\"[{datetime.now().strftime('%H:%M:%S')}] Timeout error on page {page}\")\n",
        "            # try to continue with next page\n",
        "            continue\n",
        "        except httpx.HTTPStatusError as e:\n",
        "            if e.response.status_code == 429:\n",
        "                print(f\"[{datetime.now().strftime('%H:%M:%S')}] Rate limited. Waiting 30 seconds...\")\n",
        "                time.sleep(30)\n",
        "                continue\n",
        "            else:\n",
        "                print(f\"[{datetime.now().strftime('%H:%M:%S')}] HTTP error on page {page}: {e}\")\n",
        "                continue\n",
        "        except Exception as e:\n",
        "            print(f\"[{datetime.now().strftime('%H:%M:%S')}] ERROR on page {page}: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            continue\n",
        "\n",
        "    return all_products"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLMu_s0sugd2"
      },
      "outputs": [],
      "source": [
        "# collect at least 5000 entries\n",
        "all_products = []\n",
        "target_products = 5000\n",
        "\n",
        "# loop through each category\n",
        "for idx, search_url in enumerate(search_urls):\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Scraping search query {idx+1}/{len(search_urls)}\")\n",
        "    print(f\"URL: {search_url}\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    # scrape products from this category\n",
        "    products = scrape_ebay_search(search_url, max_pages=10)\n",
        "    all_products.extend(products)\n",
        "\n",
        "    print(f\"Total products collected so far: {len(all_products)}\")\n",
        "\n",
        "    # step after reaching the target\n",
        "    if len(all_products) >= target_products:\n",
        "        print(f\"\\nReached target of {target_products} products!\")\n",
        "        break\n",
        "\n",
        "    # save intermediate results in a CSV file\n",
        "    df_intermediate = pd.DataFrame(all_products)\n",
        "    df_intermediate.to_csv(f'ebay_products_intermediate_{idx+1}.csv', index=False)\n",
        "\n",
        "    # longer random delay between different searches\n",
        "    wait_time = random.uniform(10, 15)\n",
        "    print(f\"Waiting {wait_time:.1f} seconds before next search query...\")\n",
        "    time.sleep(wait_time)\n",
        "\n",
        "# final DataFrame\n",
        "df = pd.DataFrame(all_products)\n",
        "print(f\"\\nFinal dataset shape: {df.shape}\")\n",
        "print(f\"Total products collected: {len(df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9jiS9WLuh3O"
      },
      "outputs": [],
      "source": [
        "# remove duplicates\n",
        "df = df.drop_duplicates(subset=['title', 'price', 'link'])\n",
        "print(f\"After removing duplicates: {len(df)} products\")\n",
        "\n",
        "# create a combined text field for better search\n",
        "df['combined_text'] = df.apply(lambda row: f\"{row['title']} {row['description']} {row['condition']}\", axis=1)\n",
        "\n",
        "# first few entries\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NV0m_RK9ui--"
      },
      "outputs": [],
      "source": [
        "# save dataset into a CSV\n",
        "df.to_csv('ebay_products_dataset.csv', index=False)\n",
        "print(\"Dataset saved to 'ebay_products_dataset.csv'\")\n",
        "\n",
        "# save a text file with just the combined text for semantic search\n",
        "with open('ebay_products_text.txt', 'w', encoding='utf-8') as f:\n",
        "    for text in df['combined_text']:\n",
        "        f.write(text + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NtEIODQuk58"
      },
      "outputs": [],
      "source": [
        "# load a pre-trained sentence transformer model for embedding\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "print(\"Sentence transformer model loaded successfully\")\n",
        "\n",
        "# test the model\n",
        "test_embedding = model.encode(\"test sentence\")\n",
        "print(f\"Embedding dimension: {test_embedding.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGvSZ_lgul_K"
      },
      "outputs": [],
      "source": [
        "# create embeddings for all products\n",
        "embeddings = model.encode(df['combined_text'].tolist(), show_progress_bar=True)\n",
        "print(f\"Embeddings shape: {embeddings.shape}\")\n",
        "\n",
        "# save embeddings (NumPy array) to reload later\n",
        "np.save('ebay_product_embeddings.npy', embeddings)\n",
        "print(\"Embeddings saved to 'ebay_product_embeddings.npy'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URs5ydSFum9i"
      },
      "outputs": [],
      "source": [
        "# create FAISS index that uses L2 distance (Euclidean) to measure similarity between vectors\n",
        "dimension = embeddings.shape[1]\n",
        "print(dimension) # for all-MiniLM-L6-v2 this is 384\n",
        "index = faiss.IndexFlatL2(dimension) # embeddings are normalized, so L2 distance can still reflect semantic similarity\n",
        "\n",
        "# add item embeddings to the index\n",
        "index.add(embeddings.astype('float32'))\n",
        "\n",
        "# save the index to a binary file for reloading later\n",
        "faiss.write_index(index, \"ebay_product_index.bin\")\n",
        "print(f\"FAISS index created with {index.ntotal} vectors\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdRRq1OmIQbo"
      },
      "source": [
        "# RAG"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai"
      ],
      "metadata": {
        "id": "b8vIyAg6zYpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUAQXC5kLr-Z"
      },
      "outputs": [],
      "source": [
        "# search system that uses RAG for query enhancement and result refinement\n",
        "class RAGEnhancedSearchSystem:\n",
        "    def __init__(self, embedding_model, index, df, api_key):\n",
        "        self.embedding_model = embedding_model\n",
        "        self.index = index\n",
        "        self.df = df\n",
        "\n",
        "        # initialize generative model for RAG-based search enhancement\n",
        "        self.client = genai.Client(api_key=\"AIzaSyC7JRFZVAyVuKtreWJGdHDjYHFAIyPbBuo\")\n",
        "        self.generator_model_name = \"gemini-2.0-flash\"\n",
        "        print(f\"Loading generative model for RAG search: {self.generator_model_name}\")\n",
        "\n",
        "        # move to GPU if available\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "    # use RAG to expand the search query for better retrieval\n",
        "    def rag_query_expansion(self, original_query):\n",
        "\n",
        "        expansion_terms = []\n",
        "\n",
        "        prompt = f\"\"\"Generate 3-5 related search terms for this eBay search query.\n",
        "Only output the new terms, separated by commas. Do not repeat the original query.\n",
        "\n",
        "Query: {original_query}\n",
        "\n",
        "Related terms:\"\"\"\n",
        "\n",
        "        enhanced_terms_response = self.client.models.generate_content(\n",
        "            model=self.generator_model_name,\n",
        "            contents=prompt,\n",
        "        )\n",
        "\n",
        "        enhanced_terms = enhanced_terms_response.text\n",
        "\n",
        "        # enhanced_terms = self.generator_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        if enhanced_terms and enhanced_terms.lower() != original_query.lower():\n",
        "            expansion_terms = enhanced_terms.split(',')\n",
        "\n",
        "        # clean and format expansion terms\n",
        "        cleaned_terms = [term.strip() for term in expansion_terms if term.strip()]\n",
        "        expansion_text = ' '.join(cleaned_terms[:3])  # use top 3 terms\n",
        "\n",
        "        # combine original query with expansion terms\n",
        "        if expansion_text:\n",
        "            combined_query = f\"{original_query} {expansion_text}\"\n",
        "        else:\n",
        "            combined_query = original_query\n",
        "\n",
        "        return combined_query, expansion_text\n",
        "\n",
        "    # use RAG to rerank search results based on relevance\n",
        "    def rag_result_reranking(self, original_query, search_results):\n",
        "        # prepare context with search results\n",
        "        context = \"Search results to analyze:\\n\\n\"\n",
        "        for i, result in enumerate(search_results, 1):\n",
        "            context += f\"Result {i}:\\n\"\n",
        "            context += f\"Title: {result['title']}\\n\"\n",
        "            context += f\"Price: {result['price']}\\n\"\n",
        "            if 'condition' in result:\n",
        "                context += f\"Condition: {result['condition']}\\n\"\n",
        "            if 'description' in result and result['description']:\n",
        "                context += f\"Description: {result['description'][:100]}...\\n\"\n",
        "            context += \"\\n\"\n",
        "\n",
        "        prompt = f\"\"\"You are a search result analyzer. Rerank these eBay search results based on relevance to the query.\n",
        "\n",
        "Query: {original_query}\n",
        "\n",
        "{context}\n",
        "\n",
        "Instructions:\n",
        "1. Analyze each result's relevance to the query\n",
        "2. Consider product specifics, price range, and condition\n",
        "3. Output just the ranking as numbers separated by commas, e.g. \"1,2,3...\" in order of relevance, don't add any comments\n",
        "\n",
        "Result ranking (most relevant first):\"\"\"\n",
        "\n",
        "        ranking_text_response = self.client.models.generate_content(\n",
        "            model=self.generator_model_name,\n",
        "            contents=prompt,\n",
        "        )\n",
        "\n",
        "        ranking_text = ranking_text_response.text\n",
        "\n",
        "        # parse ranking\n",
        "        try:\n",
        "            rankings = [int(x) for x in ranking_text.split(',')]\n",
        "            reranked_results = []\n",
        "            for rank in rankings:\n",
        "                if 1 <= rank <= len(search_results):\n",
        "                    result = search_results[rank - 1].copy()\n",
        "                    result['rag_rank'] = len(reranked_results) + 1\n",
        "                    reranked_results.append(result)\n",
        "\n",
        "            # add any missing results\n",
        "            for i, result in enumerate(search_results):\n",
        "                if not any(r['title'] == result['title'] for r in reranked_results):\n",
        "                    result_copy = result.copy()\n",
        "                    result_copy['rag_rank'] = len(reranked_results) + 1\n",
        "                    reranked_results.append(result_copy)\n",
        "\n",
        "            return reranked_results\n",
        "        except:\n",
        "            # fallback to original order if parsing fails\n",
        "            print(\"Failed to parse RAG ranking, using original order\")\n",
        "            return search_results\n",
        "\n",
        "    # perform search using RAG for query enhancement and result reranking\n",
        "    def rag_enhanced_search(self, query, top_k=5):\n",
        "        print(f\"\\nRAG-ENHANCED SEARCH PROCESS:\")\n",
        "        print(f\"Original query: '{query}'\")\n",
        "\n",
        "        # 1. RAG Query Expansion\n",
        "        expanded_query, enhancement_terms = self.rag_query_expansion(query)\n",
        "        print(f\"Enhanced query: '{expanded_query}'\")\n",
        "        print(f\"Enhancement terms: '{enhancement_terms}'\")\n",
        "\n",
        "        # 2. Search with expanded query\n",
        "        query_embedding = self.embedding_model.encode([expanded_query])\n",
        "        distances, indices = self.index.search(query_embedding.astype('float32'), top_k * 2)  # get extra results\n",
        "\n",
        "        # 3. format initial results\n",
        "        initial_results = []\n",
        "        for i, idx in enumerate(indices[0]):\n",
        "            if idx != -1:\n",
        "                product = self.df.iloc[idx]\n",
        "                initial_results.append({\n",
        "                    'rank': i + 1,\n",
        "                    'title': product['title'],\n",
        "                    'price': product['price'],\n",
        "                    'condition': product['condition'],\n",
        "                    'description': product['description'],\n",
        "                    'link': product['link'],\n",
        "                    'similarity_score': 1 / (1 + distances[0][i])\n",
        "                })\n",
        "        print(len(initial_results))\n",
        "\n",
        "        # 4. RAG-based reranking\n",
        "        reranked_results = self.rag_result_reranking(query, initial_results)\n",
        "\n",
        "        # return top_k results\n",
        "        return reranked_results[:top_k]\n",
        "\n",
        "    # generate response using retrieved context\n",
        "    def generate_response(self, query, search_results):\n",
        "        # create context from search results\n",
        "        context = \"Top search results for your query:\\n\\n\"\n",
        "        for i, result in enumerate(search_results, 1):\n",
        "            context += f\"Product {i} (RAG Rank: {result.get('rag_rank', i)}):\\n\"\n",
        "            context += f\"- Title: {result['title']}\\n\"\n",
        "            context += f\"- Price: {result['price']}\\n\"\n",
        "            if 'condition' in result:\n",
        "                context += f\"- Condition: {result['condition']}\\n\"\n",
        "            if 'description' in result and result['description']:\n",
        "                context += f\"- Description: {result['description'][:150]}...\\n\"\n",
        "            context += f\"- Match quality: {result['similarity_score']:.2f}\\n\\n\"\n",
        "\n",
        "        # prompt with instructions\n",
        "        prompt = f\"\"\"Recommend the best product for: {query}\n",
        "\n",
        "Products available:\n",
        "{context}\n",
        "\n",
        "Write a brief recommendation. Begin with \"Based on your search for {query},\" then mention the best product by name and price. Explain why it's good. Compare to one alternative if relevant.\n",
        "\"\"\"\n",
        "\n",
        "        response = self.client.models.generate_content(\n",
        "            model=self.generator_model_name,\n",
        "            contents=prompt,\n",
        "        )\n",
        "\n",
        "        response_obj = response.text\n",
        "\n",
        "        return response_obj\n",
        "\n",
        "    # add recommendation system that recommends 1 cheap/expensive product from top 3 related categories based on the search query\n",
        "    def recommendation_system(self, query):\n",
        "      search_results = self.rag_enhanced_search(query, top_k=5)\n",
        "      # create context from search results\n",
        "      context = \"Top search results for your query:\\n\\n\"\n",
        "      for i, result in enumerate(search_results, 1):\n",
        "          context += f\"Product {i} (RAG Rank: {result.get('rag_rank', i)}):\\n\"\n",
        "          context += f\"- Title: {result['title']}\\n\"\n",
        "          context += f\"- Price: {result['price']}\\n\"\n",
        "          if 'condition' in result:\n",
        "              context += f\"- Condition: {result['condition']}\\n\"\n",
        "          if 'description' in result and result['description']:\n",
        "              context += f\"- Description: {result['description'][:150]}...\\n\"\n",
        "          context += f\"- Match quality: {result['similarity_score']:.2f}\\n\\n\"\n",
        "\n",
        "      # retrieve 10 products per category\n",
        "      category_samples = {}\n",
        "      categories = ['electronics', 'laptops', 'cameras', 'home and garden',\n",
        "                  'sporting goods', 'clothing', 'jewelry', 'video games', 'books', 'collectibles']\n",
        "\n",
        "      all_samples = \"\\n\".join(category_samples.values())\n",
        "\n",
        "      # prompt with instructions\n",
        "      prompt = f\"\"\"Top 5 recommended items: {context}. First, output the list of top 5 recommendations. Then, determine if the user want wants to buy expensive or cheap\n",
        "item by top recommendations. Finally, based on the top results for their query: \"{query}\", select three new categories from this list:\n",
        "{categories}, and **for each of the three new categories**, recommend\n",
        "one cheap/expensive item from this list: {all_samples}. Begin your recommendation with \"You may also be interested in\" then give just a list of three items by their\n",
        "category, name, and price, one item **from each of the three new cateories**. Don't add any comments.\n",
        "\"\"\"\n",
        "\n",
        "      response = self.client.models.generate_content(\n",
        "          model=self.generator_model_name,\n",
        "          contents=prompt,\n",
        "      )\n",
        "\n",
        "      response_obj = response.text\n",
        "\n",
        "      print(response_obj)\n",
        "\n",
        "      return response_obj\n",
        "\n",
        "    # demonstrate the RAG-enhanced search process step by step\n",
        "    def demonstrate_rag_search_process(self, query):\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(f\"DEMONSTRATING RAG-ENHANCED SEARCH PROCESS\")\n",
        "        print(f\"Query: '{query}'\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # 1. query enhancement\n",
        "        expanded_query, enhancement_terms = self.rag_query_expansion(query)\n",
        "        print(\"\\n1. RAG QUERY ENHANCEMENT:\")\n",
        "        print(f\"   Original: '{query}'\")\n",
        "        print(f\"   Enhanced: '{expanded_query}'\")\n",
        "        print(f\"   Added terms: '{enhancement_terms}'\")\n",
        "\n",
        "        # 2. initial search\n",
        "        print(\"\\n2. VECTOR SEARCH WITH ENHANCED QUERY:\")\n",
        "        query_embedding = self.embedding_model.encode([expanded_query])\n",
        "        distances, indices = self.index.search(query_embedding.astype('float32'), 5)\n",
        "\n",
        "        initial_results = []\n",
        "        for i, idx in enumerate(indices[0]):\n",
        "            if idx != -1:\n",
        "                product = self.df.iloc[idx]\n",
        "                result = {\n",
        "                    'rank': i + 1,\n",
        "                    'title': product['title'],\n",
        "                    'price': product['price'],\n",
        "                    'similarity_score': 1 / (1 + distances[0][i])\n",
        "                }\n",
        "                initial_results.append(result)\n",
        "                print(f\"   {i+1}. {product['title'][:50]}...\")\n",
        "                print(f\"      Price: {product['price']} | Similarity: {result['similarity_score']:.3f}\")\n",
        "\n",
        "        # 3.  RAG reranking\n",
        "        print(\"\\n3. RAG-BASED RERANKING:\")\n",
        "        reranked_results = self.rag_result_reranking(query, initial_results)\n",
        "        for i, result in enumerate(reranked_results, 1):\n",
        "            print(f\"   {i}. (Original rank: {result['rank']}, New RAG rank: {result.get('rag_rank', i)})\")\n",
        "            print(f\"      {result['title'][:50]}...\")\n",
        "            print(f\"      Price: {result['price']}\")\n",
        "\n",
        "        # 4. generate final response\n",
        "        print(\"\\n4. GENERATE FINAL RESPONSE:\")\n",
        "        response = self.generate_response(query, reranked_results)\n",
        "        print(\"\\nAI Response:\")\n",
        "        print(\"-\" * 50)\n",
        "        print(response)\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        return response\n",
        "\n",
        "# create the RAG-enhanced search system\n",
        "rag_search_system = RAGEnhancedSearchSystem(model, index, df, api_key=\"AIzaSyC7JRFZVAyVuKtreWJGdHDjYHFAIyPbBuo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3oKOKZFLyV2"
      },
      "outputs": [],
      "source": [
        "# test the system with demonstration\n",
        "test_queries = [\n",
        "    \"I want to buy a gaming laptop\",\n",
        "    \"Can you help me find the best basketball shoes?\",\n",
        "    \"Help me pick golden earrings\"\n",
        "]\n",
        "\n",
        "print(\"\\nTESTING RAG-ENHANCED SEARCH SYSTEM\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for query in test_queries:\n",
        "    results = rag_search_system.demonstrate_rag_search_process(query)\n",
        "    print(\"\\n\" + \"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sN3lrlmbMMdV"
      },
      "outputs": [],
      "source": [
        "# interactive RAG-enhanced search\n",
        "def interactive_rag_search_demo():\n",
        "    print(\"\\nINTERACTIVE RAG-ENHANCED SEARCH DEMO\")\n",
        "    print(\"Type 'quit' to exit\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    while True:\n",
        "        query = input(\"\\nEnter your search query: \").strip()\n",
        "\n",
        "        if query.lower() in ['quit', 'exit', \"q\"]:\n",
        "            break\n",
        "\n",
        "        if query:\n",
        "            rag_search_system.demonstrate_rag_search_process(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFecFh2MMX8f"
      },
      "outputs": [],
      "source": [
        "# run interactive demo\n",
        "interactive_rag_search_demo()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "# priklausomai nuo vartotojo uzklausos ir norimos prekes LLM pagalba gauti top 3 kitas kategorijas bei prabangia ar pigia preke vartotojas nori pirkti. Ir pagal tai pateikti\n",
        "# rekomendacijas top surasto ir tada top duotose kategorijose priklausomai, ar pigi ar prabangi preke, rasti pigiausias ir brangiausias toje kategorijoje (po viena)\n"
      ],
      "metadata": {
        "id": "SK25u3yevWMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# interactive RAG-enhanced search\n",
        "# give cheap/expensive recommendations from top 3 relevant categories based on customer's query:\n",
        "def interactive_rag_search_with_recommendation():\n",
        "    print(\"\\nINTERACTIVE RAG-ENHANCED SEARCH WITH RECOMMENDATIONS\")\n",
        "    print(\"Type 'quit' to exit\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    while True:\n",
        "        query = input(\"\\nEnter your search query: \").strip()\n",
        "\n",
        "        if query.lower() in ['quit', 'exit', \"q\"]:\n",
        "            break\n",
        "\n",
        "        if query:\n",
        "            rag_search_system.recommendation_system(query)"
      ],
      "metadata": {
        "id": "Rhjx-aiMG_hn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interactive_rag_search_with_recommendation()"
      ],
      "metadata": {
        "id": "EU1kj1bZHUXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4PvjPjCF_zqq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMfFJlu2z0qA2WFY8fc5N2T",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}